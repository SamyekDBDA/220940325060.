# -*- coding: utf-8 -*-
"""Team7_AASModuleProject.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1QrDuKO8-eddQa4JZu1WXGXZQc0ZZQMru

#**Case Study OF Banking and Finacial Services** : Team 7
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

"""## 1. Reading the dataset and understanding it"""

# Loading Dataset : application_data
df_app=pd.read_csv('/content/drive/MyDrive/Exam docs/AAS_Project/Copy of application_data (1).csv')

# printing the first 5 records
df_app.head()

# Loading Dataset : previous_application
df_prev=pd.read_csv('/content/drive/MyDrive/Exam docs/AAS_Project/Copy of previous_application.csv')

# printing the first 5 records
df_prev.head()

# Checking the numner of rows and columns in the dataset application_data (1).csv

df_app.shape

#---Observation--
# the number of records=307511, the number of columns=122

# Checking the numner of rows and columns in the dataset previous_application.csv
df_prev.shape

#---Observation--
#  the number of records=1670214, the number of columns=37

# Checking the column data size and data types

df_app.info()

# Checking the column data size and data types
df_prev.info()

#Checking the statistical data : application_data

df_app.describe()

#Checking the statistical data : previous_application

df_prev.describe()

"""##2. Cleaning the data Set and dropping the not required columns"""

# Column-wise null count
df_app.isnull().sum()

#---Observation--
#There are multiple columns with null values present

# Column-wise null count
df_prev.isnull().sum()

#---Observation--
#There are multiple columns with null values present

"""### Dropping the columns that have more than 35% of null values """

# Cleaning the missing data
# listing the null values columns having more than 35%
emptycol=df_app.isnull().sum()
emptycol=emptycol[emptycol.values>(0.35*len(emptycol))]
len(emptycol)

#---Observation--
# There are 64 columns have more than 35% null values

# Removing those 64 columns
emptycol = list(emptycol[emptycol.values>=0.35].index)
df_app.drop(labels=emptycol,axis=1,inplace=True)
print(len(emptycol))

#Verifying the shape of the dataset after removing columns
df_app.shape

# Cleaning the missing data
# listing the null values columns having more than 35%
emptycol=df_prev.isnull().sum()
emptycol=emptycol[emptycol.values>(0.35*len(emptycol))]
len(emptycol)


#---Observation--
# There are 15 columns have more than 35% null values

# Removing those 15 columns
emptycol = list(emptycol[emptycol.values>=0.35].index)
df_prev.drop(labels=emptycol,axis=1,inplace=True)
print(len(emptycol))

#Verifying the shape of the dataset after remving columns
df_prev.shape

"""### Checking the remaining columns in both the datasets"""

# Checking the remaining columns in both the datasets

df_app.info()

df_prev.info()

df_app.columns

"""### Creating new dataframe with required Columns"""

# Dropping unnecessary columns from the application dataset
df_newapp=df_app.drop(['CNT_CHILDREN','NAME_EDUCATION_TYPE',
       'NAME_FAMILY_STATUS', 'REGION_POPULATION_RELATIVE', 'DAYS_EMPLOYED', 'DAYS_REGISTRATION', 'DAYS_ID_PUBLISH',
       'FLAG_MOBIL', 'FLAG_EMP_PHONE', 'FLAG_WORK_PHONE', 'FLAG_CONT_MOBILE',
       'FLAG_PHONE', 'FLAG_EMAIL', 'CNT_FAM_MEMBERS',
        'WEEKDAY_APPR_PROCESS_START',
       'HOUR_APPR_PROCESS_START', 'REG_REGION_NOT_LIVE_REGION',
       'REG_REGION_NOT_WORK_REGION', 'LIVE_REGION_NOT_WORK_REGION',
       'REG_CITY_NOT_LIVE_CITY', 'REG_CITY_NOT_WORK_CITY',
       'LIVE_CITY_NOT_WORK_CITY', 'ORGANIZATION_TYPE',
       'DAYS_LAST_PHONE_CHANGE', 'FLAG_DOCUMENT_2', 'FLAG_DOCUMENT_3',
       'FLAG_DOCUMENT_4', 'FLAG_DOCUMENT_5', 'FLAG_DOCUMENT_6',
       'FLAG_DOCUMENT_7', 'FLAG_DOCUMENT_8', 'FLAG_DOCUMENT_9',
       'FLAG_DOCUMENT_10', 'FLAG_DOCUMENT_11', 'FLAG_DOCUMENT_12',
       'FLAG_DOCUMENT_13', 'FLAG_DOCUMENT_14', 'FLAG_DOCUMENT_15',
       'FLAG_DOCUMENT_16', 'FLAG_DOCUMENT_17', 'FLAG_DOCUMENT_18',
       'FLAG_DOCUMENT_19', 'FLAG_DOCUMENT_20', 'FLAG_DOCUMENT_21'],axis=1)

#Verify the column names in the new dataframe

df_newapp.info()

#Verify the shape of newly created dataset
df_newapp.shape

#Dropping columns - previous application date
df_prev.columns

df_newprev=df_prev.drop(['SK_ID_PREV', 'SK_ID_CURR', 'NAME_CONTRACT_TYPE', 'AMT_APPLICATION',
      'WEEKDAY_APPR_PROCESS_START', 'HOUR_APPR_PROCESS_START',
       'FLAG_LAST_APPL_PER_CONTRACT', 'NFLAG_LAST_APPL_IN_DAY',
       'NAME_CASH_LOAN_PURPOSE', 'DAYS_DECISION',
       'NAME_PAYMENT_TYPE', 'NAME_PORTFOLIO', 'NAME_PRODUCT_TYPE',
       'CHANNEL_TYPE', 'SELLERPLACE_AREA', 'NAME_SELLER_INDUSTRY'],axis=1)

#Verifying the shape of the dataset after removing the uneccesary columns

df_newprev.shape

#Verify the column names
df_newprev.info()

"""###Checking if there are any duplicate values in both the datasets"""

#Checking if there are duplicate values
df_newapp.duplicated().sum()

# Observation: No Duplicate rows are present.

# Removing duplicate rows
df_newprev.duplicated().sum()

#---Observation--
#duplicate rows are present
#removing duplicate rows

#removing duplicate rows
df_newprev=df_newprev.drop_duplicates(keep='first')

#Verifying shape after deleting the duplicate columns
df_newprev.shape

"""### DATA PREPERATION:"""

#renaming column names in application dataset
df_newapp.columns

#As some column names are not appropriate, so renaming column names

df_newapp.columns=['ID_CURR', 'TARGET', 'NAME_CONTRACT_TYPE', 'GENDER',
       'FLAG_OWN_CAR', 'FLAG_OWN_REALTY', 'AMT_INCOME_TOTAL', 'AMT_CREDIT',
       'AMT_ANNUITY', 'NAME_INCOME_TYPE', 'NAME_HOUSING_TYPE', 'DAYS_BIRTH',
       'REGION_RATING_CLIENT', 'REGION_RATING_CLIENT_W_CITY']

#Checking the column names are successfully renamed or not
df_newapp.columns

df_newprev.columns

#all column names are appropriate in previous application dataset

#Verify datatype and change the dtypes if necessary

df_newapp.info()

#Verify datatype and change the dtypes if necessary
df_newprev.info()

# finding null values
df_newapp.isnull().sum()

#--Observations--
# 12 null values are present in the column AMT_ANNUITY

# Replacing the null values with "NAN"
df_newapp=df_newapp.replace(np.nan, 'NAN')

"""### UNDERSTANDING THE VARIABLES:"""

#To check the unique variables for NAME_CONTRACT_TYPE
print(len(df_newapp.NAME_CONTRACT_TYPE.unique()))
print(df_newapp.NAME_CONTRACT_TYPE.unique())


#--Observations--
# NAME_CONTRACT_TYPE has two unique values-'Cash loans' , 'Revolving loans'

#To check the unique variables for NAME_INCOME_TYPE
print(len(df_newapp.NAME_INCOME_TYPE.unique()))
print(df_newapp.NAME_INCOME_TYPE.unique())

#--Observations--
# NAME_INCOME_TYPE has 8 unique values-'Working','State servant','Commercial associate','Pensioner','Unemployed','Student','Businessman','Maternity leave'

#To check the unique variables for GENDER
print(len(df_newapp.GENDER.unique()))
print(df_newapp.GENDER.unique())


#--Observations--
# GENDER has 3 unique values-'M','F','XNA'

#To check the unique variables for FLAG_OWN_CAR                       
print(len(df_newapp.FLAG_OWN_CAR.unique()))
print(df_newapp.FLAG_OWN_CAR.unique())

#--Observations--
# FLAG_OWN_CAR has 2 unique values-'N','Y'

#To check the unique variables for NAME_HOUSING_TYPE                       
print(len(df_newapp.NAME_HOUSING_TYPE.unique()))
print(df_newapp.NAME_HOUSING_TYPE.unique())

#--Observations--
# NAME_HOUSING_TYPE has 6 unique values-'House / apartment','Rented apartment','With parents','Municipal apartment' ,'Office apartment','Co-op apartment'

#TO check the correlation between numerical data
df_newapp.corr()

#--Observations--
# Verifying the correlation between variables

df_newapp.describe(include='all')

#To check the unique variables for NAME_CONTRACT_STATUS
print(len(df_newprev.NAME_CONTRACT_STATUS.unique()))
print(df_newprev.NAME_CONTRACT_STATUS.unique())


#--Observations--
# NAME_CONTRACT_STATUS has 4 unique values

#To check the unique variables for NAME_CLIENT_TYPE
print(len(df_newprev.NAME_CLIENT_TYPE.unique()))
print(df_newprev.NAME_CLIENT_TYPE.unique())

#--Observations--
# NAME_CLIENT_TYPE has 4 unique values

#To check the unique variables for NAME_GOODS_CATEGORY
print(len(df_newprev.NAME_GOODS_CATEGORY.unique()))
print(df_newprev.NAME_GOODS_CATEGORY.unique())

#--Observations--
# NAME_GOODS_CATEGORY has 28 unique values

"""#  DATA VISUALIZATION : STUDY OF RELATIONSHIPS BETWEEN VARIABLES"""

#counting values for each variable
df_newapp.NAME_CONTRACT_TYPE.value_counts()

# Checking for the outliers for AMT_CREDIT in  new application dataset
import seaborn as sns
sns.boxplot(df_newapp.AMT_CREDIT)
plt.show()

# Checking for the outliers for AMT_CREDIT in new previous application dataset
import seaborn as sns
sns.boxplot(df_prev.AMT_CREDIT)
#plt.title('Distribution of Amount Credit')
plt.show()

# Inter Quartile Range- 
q1=df_newapp.quantile(0.25)
q3=df_newapp.quantile(0.75)
iqr=q3-q1
print(iqr)

#--Observations--
# Verifying the statistical dispersion and how the data is spread

# Observing the data of contract type 
pie=df_newapp.NAME_CONTRACT_TYPE.value_counts()

plt.barh(pie.index, pie,color='g')
plt.title("Count of Name Contract Type", fontdict={"fontsize":10}, pad =10)
#--Observation--
# More number of people are applying for the cash loans as compared to revolving loans

# Plotting a percentage graph having each category of "NAME_INCOME_TYPE"

plt.figure(figsize = [12,7])
(df_newapp["NAME_INCOME_TYPE"].value_counts()).plot.barh(color= "orange",width = .8)
plt.title("Percentage of Clients' Income Type", fontdict={"fontsize":20}, pad =20)
plt.show()


#--Observation--
# The clients who are taking the more number of loans are in the "working" income type, followed by "commercial asscociate" income type.

# Observing the AMT_INCOME_TOTAL
import matplotlib as plt
from matplotlib import pyplot
plt.pyplot.hist(df_newapp["AMT_INCOME_TOTAL"])

# plotting density of TARGET
sns.distplot(df_newapp['TARGET'])
# Observations: No.of people defaulting in the first Y installments are less compared to people defaulting for other reasons.

# Analysing all Variables using Scatter plot
sns.pairplot(data=df_newapp)

#--Observation--
#Observing the Spreading of all datapoints of all variabls

#heatplot
corr=df_newapp.corr() 
corr.style.background_gradient(cmap='coolwarm')

#To Analyze the correlation between all variables.

# Calculating Imbalance percentage
100*(df_newapp.TARGET.value_counts())/ (len(df_newapp))

# Checking for the outliers for
import seaborn as sns
import matplotlib.pyplot as plt

sns.boxplot(df_newapp.AMT_INCOME_TOTAL)
plt.title('Distribution of Income Amount')
plt.show()
# There are outliers when we check Income Amount of Client